Is a notation that describes the upper bound of the growth of the time complexity of an algorithm. It provides a way to describe how the number of operations required by and algorithm increases with the size of its input.

It describes how the runtime of an algorithm grows as the input grows. Big O deals with the worst case scenario.

If you have an algorithm that divides an array by half is probably O(logN).
If you have an algorithm that divides an array by half and scans one of those halves, it probably is O(NlogN).
